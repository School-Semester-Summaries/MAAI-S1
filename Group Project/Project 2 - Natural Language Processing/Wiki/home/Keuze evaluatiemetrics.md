# Versies

* [Versie 1](#versie-1-voor-ontwerp-en-tech-review-15-december)
* [Versie 2](#versie-2-voor-eindoplevering)

## **Versie 1: Voor ontwerp en tech review 15 december**

De prestaties van het taalmodel zullen geëvalueerd worden met verschillende kwaliteitscriteria. Eerst is bepaald welke prestatiemaat wordt gebruikt om iteraties te vergelijken. Voor dit onderzoek is een prestatiemaat (performance metric) nodig die specifiek inzicht geeft in de begrijpelijkheid van herschreven teksten op B1-niveau. Het doel is om vast te stellen of gemeentelijke brieven na vereenvoudiging daadwerkelijk leesbaar zijn en voldoet aan de regels in de OVER-schrijfwijzer. Daarom worden in dit onderzoek verschillende prestatiematen in overweging genomen voor het beoordelen van de begrijpelijkheid van gemeentelijke brieven:

* **LiNT-II** (Linguïstisch Instrumentarium voor Tekstleesbaarheid II)
* **SARI** (System output Against References and against the Input)
* **FKBLEU** (Flesch-Kincaid Bilingual Evaluation Understudy)

LiNT-II is een leesbaarheidsinstrument dat specifiek ontwikkeld is voor Nederlandstalige teksten. Deze metric berekent de moeilijkheidsgraad van een tekst aan de hand van vier taalkundige kenmerken: woordfrequentie, syntactische afhankelijkheidslengte, het aantal inhoudswoorden per zin en het aandeel concrete zelfstandige naamwoorden. Omdat LiNT-II specifiek is ontwikkeld voor Nederlandstalige teksten, sluit deze prestatiemaat goed aan bij de context van Nederlandse gemeentelijke communicatie. Daarnaast is LiNT-II empirisch onderbouwd met behulp van studies waarbij teksten beoordeeld zijn door docenten en scholieren. Hierdoor is de metric geschikt om te bepalen of teksten dichter bij het beoogde B1-niveau komen.

SARI is een veelgebruikte prestatiemaat voor tekstversimpeling, maar richt zich vooral op de manier waarop teksten worden aangepast ten opzichte van het origineel. Daarmee is SARI vooral bedoeld om de kwaliteit van vereenvoudiging te meten en niet de feitelijke leesbaarheid voor eindgebruikers, vandaar dat deze minder geschikt is voor dit onderzoek.

FKBLEU combineert een maat voor de parafrasekwaliteit (iBLEU) met een klassieke leesbaarheidsindex, Flesch-Kincaid Grade Level (FKGL). In onderzoek wordt aangetoond dat FKBLEU redelijke correlaties met menselijke beoordelingen kan geven, maar dat SARI beter samenhangt met menselijke oordelen over eenvoud en minder snel conservatieve systemen beloont die weinig aan de input veranderen.

Aangezien SARI zich niet richt op de feitelijke leesbaarheid van teksten, en FKBLEU bovendien minder goed overeenkomt met menselijke oordelen over eenvoud, worden deze twee prestatiematen niet gebruikt voor het beoordelen van de prestaties van de AI-tool. Op basis van de hierboven besproken overwegingen wordt **LiNT-II geselecteerd als primaire prestatiemaat**, omdat deze metric het meest geschikt is om de leesbaarheid van Nederlandstalige gemeentelijke teksten te beoordelen.

**Alle suggesties worden geaccepteerd, waarna de LiNT-II-score wordt berekend en vergeleken met een brief en het nulmodel.**

Naast LiNT-II wordt alleen het uiteindelijke, best presterende model beoordeeld op aanvullende kwaliteitscriteria: robuustheid, uitlegbaarheid, modelcomplexiteit en resource demand (Tabel 1). Het model wordt niet beoordeeld op schaalbaarheid, waarmee wordt bedoeld in hoeverre de AI-tool inzetbaar is bij andere gemeenten om ook daar brieven volgens de richtlijnen van de OVER-schrijfwijzer leesbaar te maken. Dit is in dit onderzoek niet mogelijk, omdat uitsluitend brieven van de OVER-gemeenten beschikbaar zijn en de AI-tool daardoor niet kan worden getest op brieven van andere gemeenten.

---

**Tabel 1 — Overzicht van aanvullende kwaliteitscriteria voor het uiteindelijke model**

| **Kwaliteitscriterium** | **Evaluatiemethode** |
|-------------------------|----------------------|
| Robuustheid | Er wordt onderzocht of de LiNT-II-score van de verbeterde brieven consistent is over verschillende briefcategorieën (Schuldhulp, WMO, Participatiewet (PW), Participatiefonds (PF) en Overig). Dit toetst of het model niet alleen voor één type tekst goed presteert (FR06). De resultaten worden weergegeven als gemiddelde LiNT-II-scores per briefcategorie, en er wordt gecontroleerd of deze scores per categorie gelijk zijn. |
| Uitlegbaarheid | Geëvalueerd wordt of de UI-elementen duidelijk maken dat de tool ondersteunend is, fouten kan maken en dat de medewerker de eindbeslissing neemt (ER01, ER02, ER04, ER05, FR01–FR03, TR02). Dit wordt getest met het prototype en met testpersonen. |
| Modelcomplexiteit | De gebruikte LLM-architectuur (Granite-3.3-2B in de naïeve RAG) wordt beschreven in termen van het aantal parameters en de modelopbouw, om de ‘zwaarte’ van het model voor de gemeentelijke infrastructuur te beoordelen. Omdat de AI-tool lokaal moet kunnen draaien, mag het model niet te computationeel belastend zijn (TR05). De complexiteit wordt daarom vergeleken met die van andere tekstversimpelingsmodellen. |
| Resource demand | Per brief worden de verwerkingstijd en het geheugengebruik gemeten. De AI-tool moet binnen een acceptabele tijd van circa twee seconden suggesties kunnen genereren, zodat de tool in de praktijk bruikbaar is zonder de werkstroom te vertragen (TR06). |

## **Versie 2: Voor eindoplevering**

De prestaties van het taalmodel zullen geëvalueerd worden met verschillende kwaliteitscriteria. Eerst is bepaald welke prestatiemaat wordt gebruikt om iteraties te vergelijken. Voor dit onderzoek is een prestatiemaat (performance metric) nodig die specifiek inzicht geeft in de begrijpelijkheid van herschreven teksten. Het doel is om vast te stellen of gemeentelijke brieven na vereenvoudiging daadwerkelijk leesbaar zijn en voldoen aan de regels in de OVER-schrijfwijzer. Daarom worden in dit onderzoek meerdere prestatiematen overwogen, waaronder de tien meest gebruikte metrics voor het beoordelen van versimpelde teksten uit \\cite{munoz2025exploring}, aangevuld met de LiNT-II-score en FKBLEU, om de eenvoud van gemeentelijke brieven te beoordelen. Tabel\~\\ref{tab:metrics} geeft een overzicht van de overwogen prestatiematen, inclusief een korte toelichting op hun relevantie voor dit onderzoek.

| Prestatiemaat | Beschrijving |
|---------------|--------------|
| SARI | SARI is een veelgebruikte prestatiemaat voor tekstversimpeling die zich richt op de manier waarop een vereenvoudigde tekst afwijkt van het origineel \[^sari_fkbleu\]. Omdat SARI altijd een originele zin en een corresponderende vereenvoudigde versie nodig heeft om een score te berekenen, kan de metric niet worden toegepast op enkel de originele gemeentelijke brieven. Bovendien meet SARI vooral de kwaliteit van de vereenvoudiging zelf en niet de feitelijke leesbaarheid voor eindgebruikers, waardoor de metric minder geschikt is voor dit onderzoek. |
| FKGL | De Flesch-Kincaid Grade Level (FKGL) meet leesbaarheid en complexiteit op basis van woord- en zinslengte. Omdat deze metriek is ontwikkeld voor Engelstalige teksten \[^kincaid1975readability\], is de toepasbaarheid op Nederlandstalige teksten beperkt. Hierdoor is deze prestatiemaat minder geschikt voor dit onderzoek. |
| BLEU | BLEU is ontworpen voor het evalueren van machinevertalingen en meet n-gram-overlap met referentieteksten. Het richt zich op vloeiendheid en behoud van inhoud in plaats van eenvoud, wat van belang is voor dit onderzoek \[^munoz2025exploring\]. Vandaar dat deze prestatiemaat minder geschikt is voor dit onderzoek. |
| FKBLEU | FKBLEU combineert een maat voor parafrasekwaliteit (iBLEU) met de leesbaarheidsindex Flesch-Kincaid Grade Level (FKGL) \[^sari_fkbleu\]. De metriek probeert zowel betekenisbehoud als vereenvoudiging te meten. Omdat de FKGL is ontwikkeld voor Engelstalige teksten \[^kincaid1975readability\], is de toepasbaarheid van FKBLEU op Nederlandstalige teksten ook beperkt. Hierdoor is deze prestatiemaat minder geschikt voor dit onderzoek. |
| Precision | Met behulp van precision kan je meten hoe beknopt een tekst is en hoeveel overbodige woorden aanwezig zijn \[^barbella2022rouge\]. Deze metriek is daardoor vooral geschikt voor samenvattingstaken en minder geschikt voor het beoordelen van de eenvoud en leesbaarheid van gemeentelijke brieven. Vandaar dat deze niet gebruikt zal worden in dit onderzoek. |
| Recall | Met Recall kan gemeten worden in welke mate de inhoud van de referentietekst behouden blijft \[^barbella2022rouge\]. Het meet dus niet of tekst eenvoudiger of beter leesbaar is. Deze metriek is daardoor vooral geschikt voor het beoordelen van samenvattingstaken en minder geschikt voor dit onderzoek. |
| F1-Score | De F1-score is het harmonisch gemiddelde van precision en recall. Aangezien zowel precision als recall minder geschikt zijn voor het beoordelen van eenvoud en leesbaarheid, wordt de F1-score ook niet gebruikt voor dit onderzoek. |
| BERT-score | De BERT-score meet vooral of de betekenis behouden blijft tussen een referentietekst en een gegenereerde tekst \[^munoz2025exploring\]. Voor het berekenen van de BERT-score is daarom altijd een herschreven tekst nodig \[^zhang2019bertscore\]. Omdat er geen standaardherschrijvingen van de originele brieven beschikbaar zijn, kan de BERT-score niet toegepast worden op de brieven zelf. Daarom is deze metriek minder geschikt voor dit onderzoek. |
| Rouge | Rouge meet hoeveel woorden of woordgroepen de gegenereerde tekst gemeen heeft met de originele tekst \[^barbella2022rouge\]. Voor samenvattingen is dit nuttig, maar voor tekstvereenvoudiging minder geschikt. Bij vereenvoudigen willen we dat zinnen makkelijker worden en de betekenis behouden blijft, zonder dat exacte woorden hetzelfde zijn. Daarom geeft Rouge niet goed weer of een tekst leesbaarder of begrijpelijker is en wordt het in dit onderzoek niet gebruikt. |
| FRES-score | De Flesch Reading Ease Score (FRES) is een leesbaarheidsmetriek die tekstcomplexiteit inschat op basis van zinslengte en het aantal lettergrepen per woord \[^flesch1948yardstick\]. De metriek is ontwikkeld en afgestemd voor Engelstalige teksten en niet formeel gevalideerd voor het beoordelen van Nederlandstalige (juridisch-administratieve) communicatie. Om deze redenen is FRES niet opgenomen als evaluatiemetric voor dit onderzoek. |
| Accuracy | Accuracy geeft het aandeel correct geclassificeerde uitkomsten weer. Aangezien tekstvereenvoudiging een generatieve taak is zonder eenduidige ‘juiste’ uitkomst, is deze metriek niet geschikt voor het beoordelen van herschreven brieven. |
| LiNT-II | LiNT-II is een leesbaarheidsinstrument dat specifiek is ontwikkeld voor Nederlandstalige teksten \[^lint_ii\]. De metric bepaalt de moeilijkheidsgraad van een tekst op basis van vier taalkundige kenmerken: woordfrequentie, syntactische afhankelijkheidslengte, het aantal inhoudswoorden per zin en het aandeel concrete zelfstandige naamwoorden \[^lint_ii, ^Maat2023LiNT\]. Omdat LiNT-II is afgestemd op Nederlandstalige teksten en empirisch is gevalideerd met beoordelingen door docenten en scholieren, is deze metric geschikt om te beoordelen in hoeverre teksten voldoen aan de richtlijnen. |

Op basis van de overwegingen in Tabel\~\\ref{tab:metrics} is LiNT-II gekozen als primaire prestatiemaat, omdat deze het meest geschikt is om de leesbaarheid van Nederlandstalige gemeentelijke teksten te beoordelen. De gemiddelde LiNT-II-score van de herschreven brieven in de validatie-set wordt berekend op een schaal van 0 tot 100. Op een aparte dataset is gekeken naar de samenhang tussen LiNT-II en het CEFR-leesniveau (Bijlage \~\\ref{app:samenhangCEFRLiNT-II}). Voor B1-teksten lag 50% van de LiNT-II-scores tussen 36.2 en 50.1, wat een indicatieve richtlijn vormt voor gemeentelijke brieven. Het is echter belangrijk te vermelden dat CEFR-annotaties niet volledig betrouwbaar zijn. Sommige teksten in de aparte dataset kregen namelijk uiteenlopende niveaus toegewezen, soms met één of twee niveaus verschil \\cite{breuker2022cefr}, waardoor dit bereik slechts als indicatieve richtlijn kan worden beschouwd.

De LiNT-II-score wordt niet aan gebruikers van de AI-tool getoond, maar dient als intern instrument om de leesbaarheid objectief te kwantificeren. Het doel van de tool is medewerkers te ondersteunen bij het schrijven van duidelijke brieven, zonder hen te belasten met numerieke scores, die hun oordeel of schrijfgedrag zou kunnen beïnvloeden. Door de score achter de schermen te gebruiken, kan het model verbeterd worden zonder de gebruiker te overladen met technische details of complexiteit die voor praktische toepassing niet relevant zijn.

Naast LiNT-II wordt alleen het best presterende model beoordeeld op robuustheid, uitlegbaarheid, modelcomplexiteit en resource demand (Tabel\~\\ref{tab:kwaliteitscriteria}). Het model wordt niet beoordeeld op schaalbaarheid, waarmee wordt bedoeld in hoeverre de AI-tool inzetbaar is bij andere gemeenten om ook daar brieven volgens de richtlijnen van de OVER-schrijfwijzer leesbaar te maken. Dit is in dit onderzoek niet mogelijk, omdat uitsluitend brieven van de OVER-gemeenten beschikbaar zijn en de AI-tool daardoor niet kan worden getest op brieven van andere gemeenten.

| Kwaliteitscriterium | Evaluatiemethode |
|---------------------|------------------|
| Robuustheid | Er wordt onderzocht of de LiNT-II-score van de verbeterde brieven consistent is over verschillende briefcategorieën (Schuldhulp, WMO, Participatiewet (PW), Participatiefonds (PF) en Overig). Dit toetst of het model niet alleen voor één type tekst goed presteert (RQ08). De resultaten worden weergegeven als gemiddelde LiNT-II-scores per briefcategorie. Vervolgens wordt gecontroleerd of deze scores per categorie gelijk zijn. |
| Uitlegbaarheid | Geëvalueerd wordt of de UI-elementen duidelijk maken dat de tool ondersteunend is, fouten kan maken en dat de medewerker de eindbeslissing neemt (RQ01, RQ02, RQ04). Dit wordt getest met het prototype en met testpersonen. |
| Modelcomplexiteit | Aangezien de AI-tool geen persoonsgegevens mag delen met externe partijen of servers (RQ06), en hierdoor de tool lokaal gedraaid moet worden, wordt de gebruikte LLM-architectuur (Granite-3.3-2B) in de naïeve RAG beschreven. Dit gebeurt in termen van het aantal parameters en de modelopbouw, om de ‘zwaarte’ van het model voor de gemeentelijke infrastructuur te beoordelen. De complexiteit wordt vergeleken met die van andere tekstversimpelingsmodellen. |
| Resource demand | Per zin wordt de verwerkingstijd gemeten. De AI-tool moet binnen een acceptabele tijd van ongeveer twee seconden een suggestie kunnen genereren, zodat de tool in de praktijk bruikbaar is zonder de werkstroom te vertragen (RQ12). |

