Hieronder staan de notulen van de coachingsessie van 19 januari 2026. Aanvankelijk hadden we voor deze sessie geen vragen voorbereid. Op de dag zelf kwamen echter nieuwe vragen naar voren, gebaseerd op knelpunten die we tijdens het project tegenkwamen. Deze vragen hebben we die maandag met David besproken. De dikgedrukte punten geven de vragen weer. De open punten eronder de antwoorden. (Echter is er veel gezegd. Ik heb geprobeerd de belangrijkste punten te noteren.)

* We hebben feedback gehad van Michiel over het meer vergelijken van verschillende modellen. We vroegen ons af of hij denkt dat de laatste zin enkel slaat op de één na laatste zin of de gehele tekst.

  <img width="676" height="250" alt="image" src="https://github.com/user-attachments/assets/d21a443e-29c1-4c37-b519-17c2f7ea7810" />

  * David denkt dat de laatste zin niet enkel slaat op de één na laatste zin. Het meenemen van de encoder, decoder en encoder-decoder klinkt prima. Hij denkt dat er geen vervolgstappen nodig zijn, met die nuance dat – de stap hoe ga je met alle modellen naar een stap op huggingface, als we uitleggen waarom de leesplank noot modellen logisch het beste passen binnen de requirements met een goede onderbouwing hebben we genoeg modellen genoemd vindt David- We kunnen het sterker maken door te melden dat het op een andere dataset getest is en dan aan te geven welke van de 3 het beste is. De nuance zit hem erin dat het het altijd sterker maakt, als je het alle drie getest hebt op je eigen data. Ik ga ervan uit dat je het haalt als je het hebt getest op één model, maar het is natuurlijk altijd beter om ze alle drie te testen? Die één na laatste zin zit voor mij (David) meer op boven niveau. Het is meer om de keuze die er gemaakt is goed te onderbouwen.
* We hebben gevraagd waarom je alle drie de modellen nog zou runnen als je al onderbouwd een keuze hebt gemaakt:
  * Dat hangt ervan af welke requirements zwaarder wegen. Je kan het harder maken als je kan aantonen dat het goed werkt op je eigen data (alle drie). Je kan ook noteren dat de data waarop ze getest hebben vergelijkbaar is met onze data.
* We hebben onze analyse laten zien van gelabelde CEFR-data en de berekende LiNT-II-scores die daaruit naar voren kwamen. We vroegen ons af of we dit konden gebruiken in het project, aangezien de niet herschreven brieven van de gemeente al LiNT-II-scores halen in de range van de CEFR-data. Kunnen wij CEFR zo wel gebruiken?
  * LiNT-II daar zit een deel subjectieviteit in. Maar er zijn ook redelijk objectieve zaken, hoeveel afstand zit er tussen woorden, zinslengte,
  * Ik zou zeggen je ziet de dataset daar kan je wel een verhaal op maken, daar kan je een analyse op loslaten, van er is ook een relatie tussen cefr-classificatie en lint-II-scores. Op basis daarvan is het logisch dat een lagere lint-ii score de tekst simpeler is, beter begrijpelijk. Maar je kan ook kritisch zijn, want het is niet zaligmakend. LiNT-II kijkt bijvoorbeeld niet naar Nederlandse woorden.
  * Je kan hem wel omdraaien lagere LiNT score zijn zinnen wel beter, maar het is nog niet dekkend.
  * Alleen lint haalt ook beter scores bij minder zinnen in één woord.
  * We zouden de ondergrens van de boxplot kunnen gebruiken als laagste score (gezien de analyse), want B1-niveau behalen is een sterkte requirement.
  * Lint score al goed genoeg, toch kan je nog wel besluiten om in een derde iteratie te richten op leesbaarheid.
