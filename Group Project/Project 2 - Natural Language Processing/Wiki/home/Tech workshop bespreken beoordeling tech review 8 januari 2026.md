Michiel begon erg enthousiast over dat wij een erg goed rapport hadden. Zelf hadden we de volgende vragen opgesteld behorende bij een aantal leeruitkomsten. De dikgedrukte teksten zijn de antwoorden.

* B2: "Dit is goed uitgevoerd. Het plaatje fig 6 op p 20 helpt maar een UML diagram  (zoals bijv, sequence diagram) zou nog beter zijn. Ik vind jullie naive RAG keuze prima en ook dit heb ik met plezier gelezen. Alleen de leesplank-noot modellen zijn beschreven. Zijn er nog andere modellen bekeken of beoordeeld? Op Huggingface ca. 2 miljoen modellen... Beschrijf globaal  (mag ook in bijlage) hoe je tot deze keuze gekomen bent. Bekijk bijv de architectuur (encoder / decoder / encoder-decoder ) welke geschikt is voor deze taak van 'vertalen' van moelijk naar simpele tekst.  Bestudeer ook  benchmarks of maak een LLM Arena, vergelijk meerdere modellen op de test/val set en zie hoe ze scoren. Wil je bij de eindbeoordeling een op-niveau halen dan moet je hier verder naar kijken."
  * Wat is een LLM Arena die benoemd wordt bij de beoordeling van B2? **Dat je twee modellen naast elkaar vergelijkt. Je hebt dus één input en twee modellen die je gebruikt (twee keer een output). We moeten het model vergelijken met een model wat erop lijkt. We zouden twee of meerdere modellen naast elkaar kunnen vergelijken. We hebben LLM Arena behandeld in de les. (**[**https://lmarena.ai/nl**](https://lmarena.ai/nl "https://lmarena.ai/nl")**) Dit is een hele handige tool in plaats van benchmarkt, omdat een ander model op onze data het misschien minder goed doet.**
* B3: Tijdens de techworkshop leek het alsof jullie de systematische aanpak van iteraties beter beschreven wilden hebben, bijvoorbeeld in een losse paragraaf. In de beoordeling bij B3 staat echter: “De systematische aanpak is beschreven onder 8.2 . In de presentatie kwam het toekomstig werk goed naar voren. Voor eindbeoordeling herschrijven.” Moeten we wel of niet nog een losse paragraaf toevoegen? **Hij had eerst 8.2 nog niet goed gelezen voor de tech review. Toen hij het wel gelezen was vond hij het netjes. 8.2 moet nog wel herschreven worden naar ergens een samenvatting van de werkwijze. Eerst wat de eerste iteratie was en daarna de tweede.**
* C2: LiNT-II, SARI en FKBLEU zijn goed gekozen metrics. Is Flesch-Kincaid wel zo geschikt voor Nederlands? En verder ontbreekt de relatie tussen LiNT en CEFR maar in jullie document spreken jullie daar wel over..(4.3.4)  Daar zouden we nog graag bewijs voor zien. Er zijn nog heel veel andere metrics, dus ook in de bespreking kan je nog wat opnemen welke afvallen en waarom. Wil je de scores ook presenteren aan de schrijver/gebruiker? Onderzoek dit ook en hoe het kan helpen bij het schrijfproces. (Besteed hier tijd aan om de op-niveua te behouden voor de eindbeoordeling)
  * Ik neem aan dat jullie met Flesch-Kincaid, FKBleu bedoelen. **Ja**
  * Willen jullie specifiek horen dat LiNT dus specifiek getest is op Nederlands en andere scores niet? **Voor het Nederlands zie je meetal Flesch douma, hij weet niet of dat ook flesch douma bleu een ding is. We kunnen**
  * “ Er zijn nog heel veel andere metrics, dus ook in de bespreking kan je nog wat opnemen welke afvallen en waarom.” Hoeveel moeten we er opnemen/Of is het goed als we er bijvoorbeeld nog 2 benoemen die vaak worden gebruikt bij het beoordelen van teksten, maar bijvoorbeeld niet passend zijn voor het beoordelen van  tekstversimpeling? **Hij heeft een paper met 15 metrics voor specifiek Nederlandse teksten. De metrics die er nu zijn komen een beetje uit de lucht vallen. Hij stuurt een paper met meer bronnen.**